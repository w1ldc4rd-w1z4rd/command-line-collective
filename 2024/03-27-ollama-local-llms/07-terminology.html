<!DOCTYPE html>
<html lang="en">
    <head>
        
        <title>Command Line Collective</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>	
        <link href="https://fonts.googleapis.com/css2?family=Sono:wght@200..800&display=swap" rel="stylesheet">
        <link rel="icon" type="image/x-icon" href="/cfg/media/favicon.ico">
        <link rel="stylesheet" href="/cfg/css/normalize.css">
        <link rel="stylesheet" href="/cfg/css/html.css">
        <link rel="stylesheet" href="/cfg/css/custom.css">
    
    </head>
<body>
    	<main>
         
             <div style="margin-bottom: 15px;" id="image-header">
                <img src="/cfg/media/clc-logo2.png">
            </div>
            
            <p class="center">[ <a href="./">BACK...</a> ]</p>
            
    	    <h1>LLM Terms Guide</h1>

<p>By: <a href="https://github.com/w1ldc4rd-w1z4rd">w1ldc4rd-w1z4rd</a></p>

<ul>
<li><strong>AVX (Advanced Vector Extensions)</strong>: Special instructions in a computer's processor that help it do many calculations at once.</li>
<li><strong>ARM NEON</strong>: A technology in certain processors that helps them handle multimedia and signal processing faster.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: A type of language model created by Google that understands the meaning of words based on their context.</li>
<li><strong>BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)</strong>: A huge model created by Hugging Face with 176 billion parameters to understand and generate text.</li>
<li><strong>Chain-of-Thought Prompting</strong>: A way to help AI solve complicated problems by breaking them down into simpler steps.</li>
<li><strong>Chains</strong>: Series of steps or tasks that an LLM performs to complete a complex job.</li>
<li><strong>Character.AI</strong>: A website where you can chat with a bot that generates responses that sound like a human.</li>
<li><strong>Chinchilla</strong>: A model developed by DeepMind with 70 billion parameters.</li>
<li><strong>CLM (Causal Language Modeling)</strong>: A task where the model guesses the next word in a sentence.</li>
<li><strong>Cohere</strong>: A company that provides tools for developers to use language AI.</li>
<li><strong>Context Window / Context Length</strong>: The number of words or pieces of information the model looks at to make its next prediction.</li>
<li><strong>CUDA (Compute Unified Device Architecture)</strong>: A technology by NVIDIA that lets programs use a computer's graphics card for calculations.</li>
<li><strong>Decoder-only Models</strong>: Models that create responses one word at a time.</li>
<li><strong>DeepSpeed</strong>: A tool to make deep learning models work faster and more efficiently.</li>
<li><strong>Diffusion Models</strong>: Models that create outputs by gradually refining random noise into structured data.</li>
<li><strong>Distillation</strong>: Teaching a smaller model to behave like a larger, more powerful one.</li>
<li><strong>EleutherAI</strong>: A group focused on making AI more understandable and aligned with human values.</li>
<li><strong>Embedding</strong>: Turning complex data into simpler, more manageable forms.</li>
<li><strong>Encoder-Decoder Models</strong>: Models with two parts: one that understands input and another that creates output.</li>
<li><strong>Encoder-only Models</strong>: Models that turn text into a fixed-size summary or representation.</li>
<li><strong>Few-shot Prompting/Learning</strong>: Giving a model a few examples to learn from before asking it to do a task.</li>
<li><strong>Fine-tuning</strong>: Adjusting a pre-trained model to do a specific job.</li>
<li><strong>FLAN (Fine-tuned LAnguage Network)</strong>: Models that can do a wide range of tasks with just a little bit of instruction.</li>
<li><strong>Flow-based Deep Generative Models</strong>: Models that represent data distributions using reversible transformations.</li>
<li><strong>GAN (Generative Adversarial Network)</strong>: Models that create new data that resembles the data they were trained on.</li>
<li><strong>Generative AI</strong>: AI systems that can create text, images, or other types of content.</li>
<li><strong>GGML (Georgi Gerganov's Machine Learning library)</strong>: A library for running large language models efficiently.</li>
<li><strong>Glean</strong>: A company that helps teams find and use knowledge.</li>
<li><strong>GLaM (Generalist Language Model)</strong>: A model that can do many different tasks.</li>
<li><strong>Gopher</strong>: A model by DeepMind with 280 billion parameters.</li>
<li><strong>Grounding</strong>: Connecting words and phrases to real-world things and ideas.</li>
<li><strong>Hallucination</strong>: When an AI makes up answers that aren't based on its training data.</li>
<li><strong>HIP (Heterogeneous-Compute Interface for Portability)</strong>: A way to write code that can run on different types of graphics cards.</li>
<li><strong>Hybrid Inference</strong>: Using both a computer's central processor and graphics card to run models that are too big for just the graphics card.</li>
<li><strong>Hugging Face</strong>: A website with tools for training and using AI models.</li>
<li><strong>InstructGPT</strong>: Models trained to follow instructions better by learning from human feedback.</li>
<li><strong>JAX</strong>: A tool for doing machine learning research really fast.</li>
<li><strong>Knowledge Distillation</strong>: Teaching a smaller model to act like a larger one.</li>
<li><strong>LaMDA (Language Model for Dialogue Applications)</strong>: A language model by Google made for conversations.</li>
<li><strong>LangChain</strong>: A framework for building and managing apps that use large language models.</li>
<li><strong>LLaMA (Large Language Model Architecture)</strong>: A type of model that's efficient and can be scaled up.</li>
<li><strong>LLM (Large Language Model)</strong>: AI models that work with and generate text that sounds like a human wrote it.</li>
<li><strong>LoRA (Low-Rank Adaptation)</strong>: A way to fine-tune big models without needing a ton of computing power.</li>
<li><strong>Low-rank Factorization</strong>: Breaking down complex data into simpler, smaller pieces to make it easier to work with.</li>
<li><strong>LM (Language Model)</strong>: A tool that predicts the likelihood of a sequence of words.</li>
<li><strong>Memory</strong>: Keeping track of information from one step to the next in a series of tasks.</li>
<li><strong>MLM (Masked Language Modeling)</strong>: A way to train models by hiding some words and asking the model to guess them.</li>
<li><strong>Modality</strong>: The type of data, like text, images, or sound.</li>
<li><strong>NLP (Natural Language Processing)</strong>: The study of how computers can understand and use human language.</li>
<li><strong>One-shot Prompting/Learning</strong>: Giving a model one example to learn from before asking it to do a task.</li>
<li><strong>OPT (Open Pretrained Transformer)</strong>: A big language model with 175 billion parameters made by Meta.</li>
<li><strong>Orchestrator</strong>: A tool that manages how different AI models and services work together in a project.</li>
<li><strong>Ollama</strong>: A tool that makes it easier to run open-source large language models on your own computer.</li>
<li><strong>OpenCL (Open Computing Language)</strong>: A standard for writing programs that can run on different types of hardware.</li>
<li><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong>: A way to fine-tune AI models without changing a lot of parameters.</li>
<li><strong>Pruning</strong>: Cutting out parts of an AI model to make it smaller and faster.</li>
<li><strong>QLoRA</strong>: An extension of LoRA for models with quantized parameters.</li>
<li><strong>Quantization</strong>: Reducing the number of bits needed to represent a model's weights to make it faster and use less memory.</li>
<li><strong>RAG (Retrieval-Augmented Generation)</strong>: Improving language models by adding a step where they look up extra information to help with their answers.</li>
<li><strong>Self-attention</strong>: A technique that helps a model focus on the important parts of a sentence.</li>
<li><strong>Self-supervised Learning</strong>: Training an AI model using data that doesn't have labels.</li>
<li><strong>SYCL</strong>: A way to write code that can run on different kinds of computing hardware.</li>
<li><strong>Transformer</strong>: The backbone of modern language models, using self-attention to understand and generate text.</li>
<li><strong>Vulkan</strong>: A low-level way to talk to graphics cards for drawing and computing tasks.</li>
</ul>

         
            <p id="bottom" class="center">
                üñ•Ô∏è <a target="_blank" href="https://www.meetup.com/command-line-collective">Meetup</a> - 
                üíæ <a target="_blank" href="https://github.com/w1ldc4rd-w1z4rd/command-line-collective">GitHub</a>
            </p>   
    	</main>
</body>
</html>
