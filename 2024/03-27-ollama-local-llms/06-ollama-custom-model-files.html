<!DOCTYPE html>
<html lang="en">
    <head>
        
        <title>Command Line Collective</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>	
        <link href="https://fonts.googleapis.com/css2?family=Sono:wght@200..800&display=swap" rel="stylesheet">
        <link rel="icon" type="image/x-icon" href="/cfg/media/favicon.ico">
        <link rel="stylesheet" href="/cfg/css/normalize.css">
        <link rel="stylesheet" href="/cfg/css/html.css">
        <link rel="stylesheet" href="/cfg/css/custom.css">
    
    </head>
<body>
    	<main>
         
             <div style="margin-bottom: 15px;" id="image-header">
                <img src="/cfg/media/clc-logo2.png">
            </div>
            
            <p class="center">[ <a href="./">BACK...</a> ]</p>
            
    	    <h1>What is a Model File?</h1>

<p>By: <a href="https://github.com/w1ldc4rd-w1z4rd">w1ldc4rd-w1z4rd</a></p>

<p>A model file, specifically in the context of Ollama, is a blueprint that defines how to create and share models. It contains instructions and parameters that Ollama uses to set up and run a Large Language Model (LLM).</p>

<h2>Format of a Model File</h2>

<p>A model file, known as a <strong>Modelfile</strong> in Ollama. Each line in the <strong>Modelfile</strong> starts with an instruction followed by its arguments. Comments can be added using the <strong>#</strong> symbol.</p>

<h2>Key Instructions in a Modelfile</h2>

<ul>
<li><strong>FROM (Required)</strong>: Specifies the base model to use. For example, <strong>FROM llama2</strong> indicates that the model is based on LLaMA-2.</li>
<li><strong>PARAMETER</strong>: Sets various parameters for running the model, such as temperature, context window size, and more.</li>
<li><strong>TEMPLATE</strong>: Is a structured format used to organize the input and output for a language model. It defines how the interaction between the user and the model should be formatted, including placeholders for various parts of the conversation, such as system messages, user prompts, and model responses. The template helps ensure that the model receives and generates information in a consistent and structured manner. For more information on how to define and customize templates in Ollama, you can refer to the <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#template">official documentation on the TEMPLATE parameter</a>.</li>
<li><strong>SYSTEM</strong>: Specifies a system message that can be used to guide the behavior of the chat assistant.</li>
<li><strong>ADAPTER</strong>: Specifies any <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#adapter">LoRA (Low-Rank Adaptation)</a> adapters that should be used to adjust the model for specific tasks. This approach helps in refining the model's performance without needing to update all its parameters.</li>
<li><strong>LICENSE</strong>: Indicates the legal license under which the model is shared or distributed.</li>
<li><strong>MESSAGE</strong>: Used to specify message history for the model to consider when responding.</li>
</ul>

<h2>Creating a Basic Modelfile</h2>

<p>Here's an example of a simple <strong>Modelfile</strong>:</p>

<p><code>
FROM mistral
PARAMETER temperature 1
PARAMETER num_ctx 4096
SYSTEM "You are a naturopathic functional medicine doctor. Your role is to listen to symptoms described by patients and prescribe natural treatments based on your expertise in holistic and integrative medicine. You should provide advice on dietary changes, herbal supplements, and lifestyle modifications that can help improve the patient's overall health and well-being. Your responses should be informative, empathetic, and focused on natural healing methods."
</code></p>

<h2>Using the Modelfile with Ollama</h2>

<ul>
<li>Save the <strong>Modelfile</strong> content to a file, for example, named <strong>NaturalModelfile</strong>.</li>
<li>Use the Ollama CLI to create a model based on this file:</li>
</ul>

<p><code>
ollama create natural-doctor -f NaturalModelfile
</code></p>

<p>Run the model:</p>

<p><code>
ollama run natural-doctor
</code></p>

<p>Interact with the model as you would with a chatbot.</p>

<h2>Understanding Ollama Model File Parameters</h2>

<p>In Ollama's <strong>Modelfile</strong>, you use the <strong>PARAMETER</strong> instruction to set up various options that control how the model behaves. Here's a simple guide to what each parameter does, including their minimum and maximum values:</p>

<h3><strong>mirostat</strong></h3>

<ul>
<li><strong>What It Does</strong>: Helps control the variety in the text the model generates.</li>
<li><strong>Possible Values</strong>:
<ul>
<li><em>0</em>: Turned off</li>
<li><em>1</em>: Basic Mirostat</li>
<li><em>2</em>: Advanced Mirostat (Mirostat 2.0)</li>
</ul></li>
<li><strong>Min/Max</strong>: 0 to 2</li>
<li><strong>Example</strong>: <em>PARAMETER mirostat 1</em></li>
</ul>

<h3><strong>mirostat_eta</strong></h3>

<ul>
<li><strong>What It Does</strong>: Adjusts how quickly the model reacts to the text it's generating.</li>
<li><strong>Default Value</strong>: <em>0.1</em></li>
<li><strong>Min/Max</strong>: Typically, 0.01 to 1.0 (but can vary based on model needs)</li>
<li><strong>Example</strong>: <em>PARAMETER mirostat_eta 0.1</em></li>
</ul>

<h3><strong>mirostat_tau</strong></h3>

<ul>
<li><strong>What It Does</strong>: Balances how consistent vs. how varied the text is.</li>
<li><strong>Default Value</strong>: <em>5.0</em></li>
<li><strong>Min/Max</strong>: Typically, 1 to 10 (but can vary based on model needs)</li>
<li><strong>Example</strong>: <em>PARAMETER mirostat_tau 5.0</em></li>
</ul>

<h3><strong>num_ctx</strong></h3>

<ul>
<li><strong>What It Does</strong>: Determines the length of the text context that the model considers when predicting the next word or token. This parameter specifies the maximum number of words or tokens from the input that the model will use to generate its response. A larger context can help the model understand the overall meaning and produce more relevant output, but it may also require more computational resources.</li>
<li><strong>Default Value</strong>: <em>2048</em> (words)</li>
<li><strong>Min/Max</strong>: Depends on the model's capabilities, but typically ranges from 512 to 4096 or more.</li>
<li><strong>Example</strong>: <em>PARAMETER num_ctx 4096</em></li>
</ul>

<h3><strong>num_gqa</strong></h3>

<ul>
<li><strong>What It Does</strong>: Specifies the number of groups in a certain part of the model. Needed for some models.</li>
<li><strong>Min/Max</strong>: Depends on the specific model architecture.</li>
<li><strong>Example</strong>: <em>PARAMETER num_gqa 8</em></li>
</ul>

<h3><strong>num_gpu</strong></h3>

<ul>
<li><strong>What It Does</strong>: Tells how many parts of the model to run on the graphics card (GPU).</li>
<li><strong>Default Value</strong>: <em>1</em> on Mac for Metal support, <em>0</em> to turn off.</li>
<li><strong>Min/Max</strong>: 0 to the number of available GPUs</li>
<li><strong>Example</strong>: <em>PARAMETER num_gpu 50</em></li>
</ul>

<h3><strong>num_thread</strong></h3>

<ul>
<li><strong>What It Does</strong>: Sets how many processing threads to use.</li>
<li><strong>Default Value</strong>: Automatically set for best performance.</li>
<li><strong>Min/Max</strong>: 1 to the number of available CPU threads</li>
<li><strong>Example</strong>: <em>PARAMETER num_thread 8</em></li>
</ul>

<h3><strong>repeat<em>last</em>n</strong></h3>

<ul>
<li><strong>What It Does</strong>: Controls how far back the model checks to avoid repeating itself.</li>
<li><strong>Default Value</strong>: <em>64</em></li>
<li><strong>Min/Max</strong>: 0 to the value of <strong>num_ctx</strong></li>
<li><strong>Possible Values</strong>:
<ul>
<li><em>0</em>: Don't check for repeats</li>
<li><em>-1</em>: Check the entire text</li>
</ul></li>
<li><strong>Example</strong>: <em>PARAMETER repeat_last_n 64</em></li>
</ul>

<h3><strong>repeat_penalty</strong></h3>

<ul>
<li><strong>What It Does</strong>: Sets how harshly to penalize repeating words.</li>
<li><strong>Default Value</strong>: <em>1.1</em></li>
<li><strong>Min/Max</strong>: Typically, 1.0 to 2.0 (but can vary based on model needs)</li>
<li><strong>Example</strong>: <em>PARAMETER repeat_penalty 1.1</em></li>
</ul>

<h3><strong>temperature</strong></h3>

<ul>
<li><strong>What It Does</strong>: Controls how "creative" the responses are. Higher = more creative.</li>
<li><strong>Default Value</strong>: <em>0.8</em></li>
<li><strong>Min/Max</strong>: 0.1 to 2.0 (lower values are more deterministic, higher values are more random)</li>
<li><strong>Example</strong>: <em>PARAMETER temperature 0.7</em></li>
</ul>

<h3><strong>seed</strong></h3>

<ul>
<li><strong>What It Does</strong>: Sets a starting point for random generation. Useful for getting the same results each time.</li>
<li><strong>Default Value</strong>: <em>0</em></li>
<li><strong>Min/Max</strong>: 0 to the maximum integer value (typically 2^31 - 1)</li>
<li><strong>Example</strong>: <em>PARAMETER seed 42</em></li>
</ul>

<h3><strong>stop</strong></h3>

<ul>
<li><strong>What It Does</strong>: Tells the model to stop generating text when it hits certain words or phrases.</li>
<li><strong>Example</strong>: <em>PARAMETER stop "AI assistant:"</em></li>
</ul>

<h3><strong>tfs_z</strong></h3>

<ul>
<li><strong>What It Does</strong>: Reduces the chance of picking less likely words. Higher = less chance.</li>
<li><strong>Default Value</strong>: <em>1</em></li>
<li><strong>Min/Max</strong>: Typically, 0.5 to 2.0 (but can vary based on model needs)</li>
<li><strong>Example</strong>: <em>PARAMETER tfs_z 1</em></li>
</ul>

<h3><strong>num_predict</strong></h3>

<ul>
<li><strong>What It Does</strong>: Sets the maximum number of words the model will generate.</li>
<li><strong>Default Value</strong>: <em>128</em></li>
<li><strong>Min/Max</strong>: 1 to the maximum integer value (typically 2^31 - 1), with special values:
<ul>
<li><em>-1</em>: Keep generating forever</li>
<li><em>-2</em>: Fill up the text window</li>
</ul></li>
<li><strong>Example</strong>: <em>PARAMETER num_predict 42</em></li>
</ul>

<h3><strong>top_k</strong></h3>

<ul>
<li><strong>What It Does</strong>: Limits the model to considering only the top 'k' words, reducing the chance of nonsense.</li>
<li><strong>Default Value</strong>: <em>40</em></li>
<li><strong>Min/Max</strong>: 1 to the size of the model's vocabulary (typically in the thousands)</li>
<li><strong>Example</strong>: <em>PARAMETER top_k 40</em></li>
</ul>

<h3><strong>top_p</strong></h3>

<ul>
<li><strong>What It Does</strong>: Works with top-k to control how varied the text is. Higher = more variety.</li>
<li><strong>Default Value</strong>: <em>0.9</em></li>
<li><strong>Min/Max</strong>: 0 to 1 (0 = most focused, 1 = most varied)</li>
<li><strong>Example</strong>: <em>PARAMETER top_p 0.9</em></li>
</ul>

<h2>Using Parameters in a Modelfile</h2>

<p>To use these parameters in a <strong>Modelfile</strong>, just write <strong>PARAMETER</strong> followed by the name of the parameter and the value you want to set. For example:</p>

<ul>
<li><em>PARAMETER temperature 0.7</em> (Sets the creativity level)</li>
<li><em>PARAMETER num_ctx 4096</em> (Sets how much text to consider)</li>
<li><em>PARAMETER top_k 50</em> (Limits to the top 50 words)</li>
</ul>

         
            <p id="bottom" class="center">
                🖥️ <a target="_blank" href="https://www.meetup.com/command-line-collective">Meetup</a> - 
                💾 <a target="_blank" href="https://github.com/w1ldc4rd-w1z4rd/command-line-collective">GitHub</a>
            </p>   
    	</main>
</body>
</html>
