<!DOCTYPE html>
<html lang="en">
    <head>
        
        <title>Command Line Collective</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>	
        <link href="https://fonts.googleapis.com/css2?family=Sono:wght@200..800&display=swap" rel="stylesheet">
        <link rel="icon" type="image/x-icon" href="/cfg/media/favicon.ico">
        <link rel="stylesheet" href="/cfg/css/normalize.css">
        <link rel="stylesheet" href="/cfg/css/html.css">
        <link rel="stylesheet" href="/cfg/css/custom.css">
    
    </head>
<body>
    	<main>
         
             <div style="margin-bottom: 15px;" id="image-header">
                <img src="/cfg/media/clc-logo2.png">
            </div>
            
            <p class="center">[ <a href="./">BACK...</a> ]</p>
            
    	    <h1>Running LLMs Locally with Ollama</h1>

<p>By: <a href="https://github.com/w1ldc4rd-w1z4rd">w1ldc4rd-w1z4rd</a></p>

<p>Ollama is a handy tool that helps you run Large Language Models (LLMs), on your own computer. To get an LLM up and running, you typically need:</p>

<ul>
<li><strong>Model Weights</strong>: Think of these as the AI's brain. They're a set of numbers that the AI uses to make sense of language and generate text that sounds like it was written by a human.</li>
<li><strong>Configurations</strong>: These are like the AI's instruction manual. They tell the AI how it should act and what rules it should follow.</li>
<li><strong>Datasets</strong>: This is the AI's study material. It's a collection of text that the AI uses to learn about language and improve its skills.</li>
</ul>

<p>Setting up an LLM with all these parts can be tricky. But Ollama makes it much simpler by putting everything together in one package. You just need to provide some directions in a file called the Modelfile, and Ollama takes care of the rest. With Ollama, you can start using the AI for tasks like writing, chatting, or answering questions without worrying about the complex setup process.</p>

<h3>Key Features of Ollama:</h3>

<ul>
<li><strong>Wide Range of Models</strong>: Supports a variety of LLMs including LLaMA-2, CodeLLaMA, Mistral, Vicuna, and more.</li>
<li><strong>Unified Package</strong>: Bundles model weights, configuration, and data into a single package, defined by a Modelfile.</li>
<li><strong>User-Friendly</strong>: Simplifies the setup and configuration process, making it easier for users to run LLMs locally.</li>
</ul>

<h3>Installation and Setup:</h3>

<ul>
<li><strong>Download Ollama</strong>: Visit the <a href="https://ollama.ai">official Ollama website</a> and download the tool for your operating system.</li>
<li><strong>Install Ollama</strong>: For Linux, use the following command in the terminal:</li>
</ul>

<p><code>
curl https://ollama.ai/install.sh | sh
</code></p>

<p><strong>Run Ollama</strong>: Once installed, Ollama creates an API to serve the model, allowing you to interact with it directly from your local machine.</p>

<h3>Running Models Using Ollama:</h3>

<p>To run a model using Ollama, use the <strong>ollama run</strong> command in the terminal. For example, to run the LLaMA 2 model:</p>

<p><code>
ollama run llama2
</code></p>

<p>If the model is not already installed, Ollama will automatically download it before running. <a href="https://ollama.com/library">Here is a list of models available</a>.</p>

<h3>Shell Usage</h3>

<p>To pipe information to Ollama or read files using it through the command line, you will typically use the standard input/output (stdin/stdout) streams or file redirection provided by your shell environment. Here's a general way of how you might use it:</p>

<p><strong>Pipe Content to Ollama</strong>:</p>

<p>If you want to send the contents of a file or the output of another command to Ollama, you can use a pipe <strong>(|)</strong>. For example:</p>

<p><code>
cat input.txt | ollama run &lt;model-name&gt;
</code></p>

<p>Or, for output from another command:</p>

<p><code>
echo "This is a test." | ollama run &lt;model-name&gt;
</code></p>

<p><strong>Read Content from a File</strong>:</p>

<p>If Ollama allows reading from a file directly, you would use it like this:</p>

<p><code>
ollama run &lt;model-name&gt; -f input.txt
</code></p>

<p>This command assumes that Ollama has a flag for file input <strong>(-f)</strong>. Replace <strong>-f</strong> with the appropriate flag according to Ollama's documentation.</p>

<p><strong>Redirect Output to a File</strong>:</p>

<p>If you want to save the output of Ollama to a file, you can redirect the output:</p>

<p><code>
ollama run &lt;model-name&gt; &gt; output.txt
</code></p>

<p><strong>Combine Piping and Redirection</strong>:</p>

<p>You can combine piping and redirection to feed content into Ollama and save its output:</p>

<p><code>
cat input.txt | ollama run &lt;model-name&gt; &gt; output.txt
</code></p>

<h3>Other Options</h3>

<p>To list all the models available:</p>

<p><code>
ollama list
</code></p>

<p>To delete a model:</p>

<p><code>
ollama rm &lt;model name&gt;
</code></p>

<p>Connect remotely</p>

<p><code>
ssh -t username@hostname '/usr/local/bin/ollama run &lt;model name&gt;'
</code></p>

<h3>System Requirements:</h3>

<p>The system requirements for running models with Ollama vary depending on the model. Here are some examples:</p>

<ul>
<li>LLaMA 2: 7B parameters
<ul>
<li>3.8GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run llama2</strong></li>
</ul></li>
<li>Mistral: 7B parameters
<ul>
<li>4.1GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run mistral</strong></li>
</ul></li>
<li>Code LLaMA: 7B parameters
<ul>
<li>3.8GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run codellama</strong></li>
</ul></li>
<li>LLaMA 2 13B: 13B parameters
<ul>
<li>7.3GB size</li>
<li>16GB RAM required</li>
<li>command: <strong>ollama run llama2:13b</strong></li>
</ul></li>
<li>LLaMA 2 70B: 70B parameters
<ul>
<li>39GB size</li>
<li>32GB RAM required</li>
<li>command: <strong>ollama run llama2:70b</strong></li>
</ul></li>
<li>Orca Mini: 3B parameters
<ul>
<li>1.9GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run orca-mini</strong></li>
</ul></li>
<li>Vicuna: 7B parameters
<ul>
<li>3.8GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run vicuna</strong></li>
</ul></li>
<li>LLaVA: 7B parameters
<ul>
<li>4.5GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run llava</strong></li>
</ul></li>
<li>Gemma (2B): 2B parameters
<ul>
<li>1.4GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run gemma:2b</strong></li>
</ul></li>
<li>Gemma (7B): 7B parameters
<ul>
<li>4.8GB size</li>
<li>8GB RAM required</li>
<li>command: <strong>ollama run gemma:7b</strong></li>
</ul></li>
</ul>

         
            <p id="bottom" class="center">
                üñ•Ô∏è <a target="_blank" href="https://www.meetup.com/command-line-collective">Meetup</a> - 
                üíæ <a target="_blank" href="https://github.com/w1ldc4rd-w1z4rd/command-line-collective">GitHub</a>
            </p>   
    	</main>
</body>
</html>
