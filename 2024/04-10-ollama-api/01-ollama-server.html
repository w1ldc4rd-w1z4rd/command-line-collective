<!DOCTYPE html>
<html lang="en">
    <head>
        
        <title>Command Line Collective</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>	
        <link href="https://fonts.googleapis.com/css2?family=Sono:wght@200..800&display=swap" rel="stylesheet">
        <link rel="icon" type="image/x-icon" href="/cfg/media/favicon.ico">
        <link rel="stylesheet" href="/cfg/css/normalize.css">
        <link rel="stylesheet" href="/cfg/css/html.css">
        <link rel="stylesheet" href="/cfg/css/custom.css">
    
    </head>
<body>
    	<main>
         
             <div style="margin-bottom: 15px;" id="image-header">
                <img src="/cfg/media/clc-logo2.png">
            </div>
            
            <p class="center">[ <a href="./">BACK...</a> ]</p>
            
    	    <h1>Starting Ollama Server</h1>

<p>By: <a href="https://github.com/w1ldc4rd-w1z4rd">w1ldc4rd-w1z4rd</a></p>

<p>Ollama is an efficient platform that enables you to download and utilize open-source models locally, automatically sourcing these models from the most optimal providers. The platform is designed to enhance performance by automatically enabling GPU acceleration if your system has a dedicated GPU, requiring no manual configuration for this feature.</p>

<p>To start the Ollama service on your machine, use the following command:</p>

<p><code>
OLLAMA_HOST=0.0.0.0 ollama serve
</code></p>

<p>This command sets the <strong>OLLAMA_HOST</strong> environment variable to <strong>0.0.0.0</strong>, allowing Ollama to listen on all network interfaces of your computer. This setup makes the service accessible from any device connected to your local network.</p>

<p>Once started, the Ollama service is typically accessible through the default URL:</p>

<p><code>
http://localhost:11434/api/generate
</code></p>

<p>This URL serves as the endpoint for interacting with the Ollama service, where you can send requests to generate outputs based on the models it has downloaded and configured.</p>

         
            <p id="bottom" class="center">
                üñ•Ô∏è <a target="_blank" href="https://www.meetup.com/command-line-collective">Meetup</a> - 
                üíæ <a target="_blank" href="https://github.com/w1ldc4rd-w1z4rd/command-line-collective">GitHub</a>
            </p>   
    	</main>
</body>
</html>
